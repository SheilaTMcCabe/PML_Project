---
title: "Practical Machine Learning: Project"
author: "Sheila"
date: "December 27, 2015"
output: 
        html_document:
        keep_md: true
---

Introduction and Background: Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 


Goal: To use data from accelerometers to predict the manner in which people perform exercise (i.e., response variable is "classe" in the training set). 

## 1. Load the required libraries

```{r library, echo = TRUE}
library(knitr)
library(caret)
library(rattle)
library(rpart)
library(rpart.plot)
library(randomForest)
```

## 2. Load the source data

```{r load the data}
## The training data source was retrieved from location: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
trainingRaw<-read.csv("pml-training.csv", header = TRUE)
dim(trainingRaw)

## The testing data source was retrieved from location: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
testingRaw<-read.csv("pml-testing.csv", header = TRUE)
dim(testingRaw)
```

## 3. Clean and partition the data

```{r process, echo = TRUE}
#remove missing values
trainingRaw1<-trainingRaw[,colSums(is.na(trainingRaw))==0]
dim(trainingRaw1)
testingRaw1<-testingRaw[,colSums(is.na(trainingRaw))==0]
dim(testingRaw1)

#remove near zero values
removeNZV<-nearZeroVar(trainingRaw1,saveMetrics=TRUE)
removeNZV1<-sum(removeNZV$nzv)
if((removeNZV1>0)){
  trainingRaw1<-trainingRaw1[,removeNZV$nzv==FALSE]
}

#remove unnecessary values, i.e., the first 7 columns provide no added value so delete
trainingClean<-trainingRaw1[,-c(1:7)]
dim(trainingClean)
testingClean<-testingRaw1[,-c(1:7)]
dim(testingClean)

#split the data
set.seed(1234)
inTrain<-createDataPartition(trainingClean$classe,p=0.7,list=FALSE)
inTrainFinal<-trainingClean[inTrain,]
dim(inTrainFinal)
inTestFinal<-trainingClean[-inTrain,]
dim(inTestFinal)
```
## 4. Develop Model

###Classification Tree

```{r predict, echo = TRUE, cache = TRUE}
modelControl<-trainControl(method="cv", 3)

#Classification Tree
modelTrain<-train(classe~.,data=inTrainFinal,method="rpart",trControl=modelControl)
modelTrain
fancyRpartPlot(modelTrain$finalModel)

predictRpart<-predict(modelTrain,inTestFinal)
confusionMatrix(inTestFinal$classe,predictRpart)
```

Classification Tree: comments

-Out-of-Sample error rate is approximately 0.5; given the Accuracy rate is approximately 0.5.

-Classification Tree does not provide a good prediction.

###Random Forest

```{r RF, echo = TRUE, cache = TRUE}
#Random Forest
modelRF<-train(classe~.,data=inTrainFinal,method="rf",trControl=modelControl)
modelRF

predictRF<-predict(modelRF,inTestFinal)
confusionMatrix(inTestFinal$classe,predictRF)
```

Random Forest: comments

-Out-of-Sample error rate is approximately 0.01; given the Accuracy rate is approximately 0.99.

-Random Forest is much better than the Classification Tree.

## 5. Run Model

Will use the Random Forest to predict the "classe" variable.

```{r predictTestSet, echo = TRUE, cache = TRUE}
runPredictions<-predict(modelRF,testingClean)
runPredictions

#Function used to generate the test case files

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(runPredictions)

```